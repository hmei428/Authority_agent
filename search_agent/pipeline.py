import logging
import os
from concurrent.futures import ThreadPoolExecutor, as_completed
from dataclasses import dataclass
from typing import Dict, Iterable, List, Optional, Set, Tuple
from urllib.parse import urlparse

import pandas as pd

from .scoring import AuthorityScorer, RelevanceScorer
from .search_client import MetaSearchClient
from .storage import StorageClient, build_output_path

logger = logging.getLogger(__name__)


@dataclass
class SearchResult:
    query: str
    query_type: Optional[str]
    url: str
    title: str
    content: str
    host: str
    search_engine: str  # æ–°å¢ï¼šæœç´¢å¼•æ“æ ‡è¯†


class AuthorityAgent:
    def __init__(
        self,
        search_client: MetaSearchClient,
        storage_client: StorageClient,
        topk: int,
        authority_threshold: int,
        relevance_threshold: int,
        max_workers: int,
        score_authority: AuthorityScorer,
        score_relevance: RelevanceScorer,
        checkpoint_interval: int = 0,  # æ–°å¢ï¼šcheckpointé—´éš”ï¼Œ0è¡¨ç¤ºä¸å¯ç”¨
        output_dir: str = "",  # æ–°å¢ï¼šè¾“å‡ºç›®å½•
        oss_paths: Optional[Dict[str, str]] = None,  # æ–°å¢ï¼šOSSè·¯å¾„é…ç½®
        enable_oss_upload: bool = False,  # æ–°å¢ï¼šæ˜¯å¦å¯ç”¨OSSä¸Šä¼ 
        oss_upload_client: Optional[StorageClient] = None,
        filter_authority_score: int = 4,
        filter_relevance_score: int = 2,
    ) -> None:
        self.search_client = search_client
        self.storage_client = storage_client
        self.topk = topk
        self.authority_threshold = authority_threshold
        self.relevance_threshold = relevance_threshold
        self.max_workers = max_workers
        self.score_authority = score_authority
        self.score_relevance = score_relevance
        self.filter_authority_score = filter_authority_score
        self.filter_relevance_score = filter_relevance_score

        # Checkpointé…ç½®
        self.checkpoint_interval = checkpoint_interval
        self.output_dir = output_dir
        self.enable_oss_upload = enable_oss_upload
        self.oss_paths = oss_paths or {}
        self.oss_upload_client = oss_upload_client

        self.authority_hosts: Dict[str, Dict[str, str]] = {}  # ä¿®æ”¹ï¼šå­˜å‚¨ {host: {"authority_score": score, "authority_reason": reason}}
        self.qna_records: List[Dict[str, str]] = []
        self.all_results_with_scores: List[Dict] = []  # æ–°å¢ï¼šå­˜å‚¨æ‰€æœ‰ç»“æœå¸¦è¯„åˆ†
        self.result_rank_counter: Dict[str, int] = {}  # æ–°å¢ï¼šè¿½è¸ªæ¯ä¸ªqueryçš„ç»“æœæ’åº
        self.metasearch_results: List[Dict] = []  # æ–°å¢ï¼šå­˜å‚¨metasearchåŸå§‹ç»“æœï¼ˆç”¨äºåç»­å•ç‹¬æ‰“åˆ†ï¼‰
        self.authority_hosts_updates: Dict[str, Dict[str, str]] = {}
        self.qna_seen_keys: Set[Tuple[str, str]] = set()
        self.csv_part_index = 0
        self.total_metasearch_records = 0
        self.total_all_results = 0
        self.total_qna_records = 0
        self.relevance_distribution_total = {0: 0, 1: 0, 2: 0}

        # ç»Ÿè®¡ä¿¡æ¯
        self.stats = {
            "total_queries": 0,
            "search_success": 0,
            "search_failed": 0,
            "authority_score_failed": 0,
            "relevance_score_failed": 0,
        }

        # Checkpointç›¸å…³
        self.checkpoint_count = 0  # checkpointåºå·

        if self.checkpoint_interval > 0 and self.output_dir:
            self.checkpoint_dir = os.path.join(self.output_dir, "checkpoints")
            os.makedirs(self.checkpoint_dir, exist_ok=True)
        else:
            self.checkpoint_dir = ""

        self._reset_chunk_state()

    def _reset_chunk_state(self) -> None:
        """æ¸…ç©ºå½“å‰æ‰¹æ¬¡ç¼“å­˜ï¼Œé‡Šæ”¾å†…å­˜"""
        self.metasearch_results = []
        self.all_results_with_scores = []
        self.qna_records = []
        self.authority_hosts_updates = {}

    def fetch_results(self, query: str, query_type: Optional[str]) -> List[SearchResult]:
        self.stats["total_queries"] += 1
        try:
            items: Iterable[Dict] = self.search_client.search(query)
            self.stats["search_success"] += 1
        except Exception as exc:  # noqa: BLE001
            self.stats["search_failed"] += 1
            logger.warning("å…ƒæœç´¢å¤±è´¥ (query=%s): %s", query, exc)
            return []

        results: List[SearchResult] = []
        for rank, item in enumerate(list(items)[: self.topk], start=1):
            url = item.get("link") or ""
            title = item.get("title") or ""
            content = item.get("content") or ""
            search_engine = item.get("search_engine") or ""
            host = urlparse(url).netloc
            if not url or not host:
                continue

            # ä¿å­˜åˆ°metasearch_resultsï¼ˆç”¨äºåç»­å•ç‹¬æ‰“åˆ†ï¼‰
            self.metasearch_results.append({
                "query": query,
                "rank": rank,
                "url": url,
                "title": title,
                "content": content,
                "host": host,
                "search_engine": search_engine,
            })
            self.total_metasearch_records += 1

            results.append(
                SearchResult(
                    query=query,
                    query_type=query_type,
                    url=url,
                    title=title,
                    content=content,
                    host=host,
                    search_engine=search_engine,
                )
            )
        return results

    def score_single_result(self, result: SearchResult, rank: int) -> Dict:
        """
        å¯¹å•ä¸ªæœç´¢ç»“æœè¿›è¡Œæ‰“åˆ†ï¼ˆå¹¶å‘å®‰å…¨ï¼‰
        åªåšæ‰“åˆ†ï¼Œä¸ä¿®æ”¹å…±äº«çŠ¶æ€
        """
        # å¯¹hostè¿›è¡Œæƒå¨æ€§æ‰“åˆ†
        try:
            authority_score, authority_reason = self.score_authority(result.host, result.title, result.content)
        except Exception as exc:  # noqa: BLE001
            logger.warning("æƒå¨æ€§æ‰“åˆ†å¤±è´¥ (host=%s): %s", result.host, exc)
            authority_score = 0
            authority_reason = ""

        # å¯¹query-contentè¿›è¡Œç›¸å…³æ€§æ‰“åˆ†
        try:
            relevance_score, relevance_reason = self.score_relevance(
                result.query, result.title, result.content
            )
        except Exception as exc:  # noqa: BLE001
            logger.warning("ç›¸å…³æ€§æ‰“åˆ†å¤±è´¥ (query=%s): %s", result.query, exc)
            relevance_score = -1
            relevance_reason = ""

        return {
            "result": result,
            "rank": rank,
            "authority_score": authority_score,
            "authority_reason": authority_reason,
            "relevance_score": relevance_score,
            "relevance_reason": relevance_reason,
        }

    def evaluate_result(self, result: SearchResult, rank: int) -> None:
        """
        è¯„ä¼°å•ä¸ªç»“æœå¹¶æ›´æ–°ç»Ÿè®¡ï¼ˆéå¹¶å‘ç‰ˆæœ¬ï¼Œä¿ç•™ç”¨äºå…¼å®¹ï¼‰
        """
        scored = self.score_single_result(result, rank)
        self._collect_scored_result(scored)

    def _collect_scored_result(self, scored: Dict) -> None:
        """
        æ”¶é›†æ‰“åˆ†åçš„ç»“æœåˆ°å…±äº«æ•°æ®ç»“æ„ï¼ˆçº¿ç¨‹å®‰å…¨ï¼Œéœ€è¦åœ¨ä¸»çº¿ç¨‹è°ƒç”¨ï¼‰
        """
        result = scored["result"]
        rank = scored["rank"]
        authority_score = scored["authority_score"]
        authority_reason = scored["authority_reason"]
        relevance_score = scored["relevance_score"]
        relevance_reason = scored["relevance_reason"]

        # æ›´æ–°å¤±è´¥ç»Ÿè®¡
        if authority_score == 0:
            self.stats["authority_score_failed"] += 1
        if relevance_score == -1:
            self.stats["relevance_score_failed"] += 1

        # å­˜å‚¨æ‰€æœ‰ç»“æœï¼ˆå¸¦è¯„åˆ†å’Œåˆ¤æ–­ä¾æ®ï¼‰
        self.all_results_with_scores.append({
            "query": result.query,
            "rank": rank,
            "url": result.url,
            "title": result.title,
            "content": result.content,
            "host": result.host,
            "search_engine": result.search_engine,
            "authority_score": authority_score,
            "authority_reason": authority_reason,
            "relevance_score": relevance_score,
            "relevance_reason": relevance_reason,
        })
        self.total_all_results += 1
        if relevance_score in self.relevance_distribution_total:
            self.relevance_distribution_total[relevance_score] += 1

        # æ”¶é›†æƒå¨hostï¼ˆåŒæ—¶å­˜å‚¨scoreå’Œreasonï¼‰
        if authority_score >= self.authority_threshold:
            existing = self.authority_hosts.get(result.host)
            if existing is None or int(existing["authority_score"]) < authority_score:
                host_entry = {
                    "authority_score": str(authority_score),
                    "authority_reason": authority_reason
                }
                self.authority_hosts[result.host] = host_entry
                self.authority_hosts_updates[result.host] = host_entry

            # æ”¶é›†é«˜æƒå¨é«˜ç›¸å…³çš„ç»“æœ
            if relevance_score >= self.relevance_threshold:
                key = (result.query, result.url)
                if key not in self.qna_seen_keys:
                    self.qna_seen_keys.add(key)
                    self.qna_records.append(
                        {
                            "query": result.query,
                            "type": result.query_type or "",
                            "url": result.url,
                            "title": result.title,
                            "content": result.content,
                            "authority_score": authority_score,
                            "relevance_score": relevance_score,
                        }
                    )
                    self.total_qna_records += 1

    def save_checkpoint(
        self,
        filter_authority_score: Optional[int] = None,
        filter_relevance_score: Optional[int] = None,
    ) -> None:
        """ä¿å­˜checkpointåˆ°æœ¬åœ°å¹¶ä¸Šä¼ OSS"""
        if self.checkpoint_interval <= 0:
            return

        if filter_authority_score is None:
            filter_authority_score = self.filter_authority_score
        if filter_relevance_score is None:
            filter_relevance_score = self.filter_relevance_score

        self.checkpoint_count += 1
        checkpoint_name = f"checkpoint_{self.checkpoint_count:03d}"
        checkpoint_path = os.path.join(self.checkpoint_dir, checkpoint_name)
        os.makedirs(checkpoint_path, exist_ok=True)

        logger.info(f"ğŸ’¾ ä¿å­˜ {checkpoint_name}...")

        try:
            # 1. ä¿å­˜ä¸‰ä¸ªparquetæ–‡ä»¶åˆ°æœ¬åœ°
            self._save_checkpoint_parquets(checkpoint_path, filter_authority_score, filter_relevance_score)

            # 2. ä¸Šä¼ åˆ°OSSï¼ˆå¦‚æœå¯ç”¨ï¼‰
            if self.enable_oss_upload:
                self._upload_checkpoint_to_oss(checkpoint_path, checkpoint_name)

            logger.info(
                "âœ… %s ä¿å­˜å®Œæˆ (ç´¯è®¡å¤„ç† query: %d)",
                checkpoint_name,
                self.stats.get("total_queries", 0),
            )

        except Exception as e:
            logger.error(f"âŒ ä¿å­˜checkpointå¤±è´¥: {e}")
            raise  # å¤±è´¥åˆ™ä¸­æ–­å¤„ç†

    def _save_checkpoint_parquets(self, checkpoint_path: str, filter_authority_score: int, filter_relevance_score: int) -> None:
        """ä¿å­˜ä¸‰ä¸ªparquetæ–‡ä»¶åˆ°æŒ‡å®šç›®å½•"""

        # æ–‡ä»¶1: all_results_with_scores.parquetï¼ˆæ‰€æœ‰å­—æ®µè½¬ä¸ºstrï¼‰
        if self.all_results_with_scores:
            df_all = pd.DataFrame(self.all_results_with_scores)
            # å°†æ‰€æœ‰å­—æ®µè½¬æ¢ä¸ºstrç±»å‹
            df_all = df_all.astype(str)
            parquet_path = os.path.join(checkpoint_path, "all_results_with_scores.parquet")
            df_all.to_parquet(parquet_path, index=False, engine='pyarrow')
            logger.info(f"  âœ“ all_results_with_scores.parquet ({len(df_all)} æ¡)")
        else:
            logger.warning("  âš ï¸  all_results_with_scores ä¸ºç©ºï¼Œè·³è¿‡")

        # æ–‡ä»¶2: authority_hosts.parquetï¼ˆæ·»åŠ authority_reasonï¼Œæ‰€æœ‰å­—æ®µè½¬ä¸ºstrï¼‰
        if self.authority_hosts_updates:
            df_hosts = pd.DataFrame([
                {
                    "host": host,
                    "authority_score": info["authority_score"],
                    "authority_reason": info["authority_reason"]
                }
                for host, info in self.authority_hosts_updates.items()
            ]).sort_values("authority_score", ascending=False)
            # å°†æ‰€æœ‰å­—æ®µè½¬æ¢ä¸ºstrç±»å‹
            df_hosts = df_hosts.astype(str)
            parquet_path = os.path.join(checkpoint_path, "authority_hosts.parquet")
            df_hosts.to_parquet(parquet_path, index=False, engine='pyarrow')
            logger.info(f"  âœ“ authority_hosts.parquet ({len(df_hosts)} ä¸ªhost)")
        else:
            logger.warning("  âš ï¸  æœ¬æ‰¹æ¬¡ authority_hosts ä¸ºç©ºï¼Œè·³è¿‡")

        # æ–‡ä»¶3: filtered_qna.parquetï¼ˆè°ƒæ•´å­—æ®µé¡ºåºï¼Œæ‰€æœ‰å­—æ®µè½¬ä¸ºstrï¼‰
        if self.all_results_with_scores:
            filtered_results = [
                {
                    "query": str(rec["query"]),
                    "url": str(rec["url"]),
                    "content": str(rec["content"]),
                    "title": str(rec["title"]),
                    "authority_score": str(rec["authority_score"]),
                    "relevance_score": str(rec["relevance_score"]),
                    "authority_reason": str(rec["authority_reason"]),
                    "relevance_reason": str(rec["relevance_reason"]),
                    "search_engine": str(rec["search_engine"]),
                }
                for rec in self.all_results_with_scores
                if rec["authority_score"] == filter_authority_score
                and rec["relevance_score"] == filter_relevance_score
            ]

            if filtered_results:
                df_filtered = pd.DataFrame(filtered_results)
                parquet_path = os.path.join(checkpoint_path, "filtered_qna.parquet")
                df_filtered.to_parquet(parquet_path, index=False, engine='pyarrow')
                logger.info(f"  âœ“ filtered_qna.parquet ({len(df_filtered)} æ¡)")
            else:
                logger.warning("  âš ï¸  filtered_qna ä¸ºç©ºï¼ˆæ— ç¬¦åˆæ¡ä»¶çš„ç»“æœï¼‰")
        else:
            logger.warning("  âš ï¸  all_resultsä¸ºç©ºï¼Œè·³è¿‡filtered_qna")

    def _upload_checkpoint_to_oss(self, checkpoint_path: str, checkpoint_name: str) -> None:
        """ä¸Šä¼ checkpointæ–‡ä»¶åˆ°OSS"""
        logger.info(f"â˜ï¸  ä¸Šä¼  {checkpoint_name} åˆ°OSS...")

        if not self.oss_upload_client:
            raise RuntimeError("å¯ç”¨äº†OSSä¸Šä¼ ï¼Œä½†æœªé…ç½®OSSå®¢æˆ·ç«¯")

        file_mappings = {
            "all_results_with_scores.parquet": self.oss_paths.get("all_results"),
            "authority_hosts.parquet": self.oss_paths.get("authority_hosts"),
            "filtered_qna.parquet": self.oss_paths.get("filtered_qna"),
        }

        for filename, oss_base_path in file_mappings.items():
            if not oss_base_path:
                logger.warning(f"  âš ï¸  æœªé…ç½® {filename} çš„OSSè·¯å¾„ï¼Œè·³è¿‡ä¸Šä¼ ")
                continue

            local_file = os.path.join(checkpoint_path, filename)
            if not os.path.exists(local_file):
                logger.warning(f"  âš ï¸  {filename} ä¸å­˜åœ¨ï¼Œè·³è¿‡ä¸Šä¼ ")
                continue

            # æ„å»ºOSSå®Œæ•´è·¯å¾„ï¼šoss://bucket/path/checkpoint_001.parquet
            oss_file_path = os.path.join(oss_base_path, f"{checkpoint_name}.parquet")

            try:
                # è¯»å–æœ¬åœ°parquet
                df = pd.read_parquet(local_file)
                # ä½¿ç”¨storage_clientä¸Šä¼ 
                self.oss_upload_client.write_parquet(df, oss_file_path)
                logger.info(f"    âœ“ {filename} â†’ {oss_file_path}")
            except Exception as e:
                logger.error(f"    âœ— ä¸Šä¼ å¤±è´¥ {filename}: {e}")
                raise  # ä¸Šä¼ å¤±è´¥åˆ™ä¸­æ–­

    def _write_csv_part(self, chunk_index: int) -> None:
        """å°†å½“å‰æ‰¹æ¬¡çš„æ•°æ®å†™å…¥åˆ†ç‰‡CSVï¼Œå¹¶é‡ç½®ç¼“å­˜"""
        if not any([self.metasearch_results, self.all_results_with_scores, self.qna_records, self.authority_hosts_updates]):
            logger.info("å½“å‰æ‰¹æ¬¡æ— æ•°æ®ï¼Œè·³è¿‡CSVè¾“å‡º")
            return

        self.csv_part_index += 1
        part_suffix = f"_part{self.csv_part_index:03d}.csv"
        os.makedirs(self.output_dir, exist_ok=True)
        logger.info("å†™å…¥CSVåˆ†ç‰‡ #%d (æ‰¹æ¬¡ç´¢å¼• %d)", self.csv_part_index, chunk_index)

        # æ–‡ä»¶0ï¼šmetasearch_results
        if self.metasearch_results:
            df_metasearch = pd.DataFrame(self.metasearch_results).astype(str)
            csv_path_0 = os.path.join(self.output_dir, f"metasearch_results{part_suffix}")
            df_metasearch.to_csv(csv_path_0, index=False, encoding="utf-8-sig")
            logger.info("âœ“ è¾“å‡ºåˆ†ç‰‡æ–‡ä»¶: %s (%d æ¡è®°å½•)", csv_path_0, len(df_metasearch))
        else:
            logger.info("å½“å‰æ‰¹æ¬¡æ— å…ƒæœç´¢ç»“æœï¼Œè·³è¿‡metasearch CSV")

        # æ–‡ä»¶1ï¼šæ‰€æœ‰ç»“æœå¸¦è¯„åˆ†
        if self.all_results_with_scores:
            df_all = pd.DataFrame(self.all_results_with_scores).astype(str)
            csv_path_1 = os.path.join(self.output_dir, f"all_results_with_scores{part_suffix}")
            df_all.to_csv(csv_path_1, index=False, encoding="utf-8-sig")
            logger.info("âœ“ è¾“å‡ºåˆ†ç‰‡æ–‡ä»¶: %s (%d æ¡è®°å½•)", csv_path_1, len(df_all))
        else:
            logger.info("å½“å‰æ‰¹æ¬¡æ— LLMæ‰“åˆ†ç»“æœï¼Œè·³è¿‡all_results CSV")

        # æ–‡ä»¶2ï¼šæƒå¨hoståˆ—è¡¨ï¼ˆä½¿ç”¨æœ¬æ‰¹æ¬¡æ›´æ–°ï¼‰
        if self.authority_hosts_updates:
            df_hosts = pd.DataFrame([
                {
                    "host": host,
                    "authority_score": info["authority_score"],
                    "authority_reason": info["authority_reason"],
                }
                for host, info in self.authority_hosts_updates.items()
            ]).sort_values("authority_score", ascending=False)
            df_hosts = df_hosts.astype(str)
            csv_path_2 = os.path.join(self.output_dir, f"authority_hosts{part_suffix}")
            df_hosts.to_csv(csv_path_2, index=False, encoding="utf-8-sig")
            logger.info("âœ“ è¾“å‡ºåˆ†ç‰‡æ–‡ä»¶: %s (%d ä¸ªhost)", csv_path_2, len(df_hosts))
        else:
            logger.info("å½“å‰æ‰¹æ¬¡æ— æ–°å¢æƒå¨hostï¼Œè·³è¿‡authority_hosts CSV")

        # æ–‡ä»¶3ï¼šç­›é€‰åçš„é«˜è´¨é‡ç»“æœ
        filtered_results = [
            {
                "query": str(rec["query"]),
                "url": str(rec["url"]),
                "content": str(rec["content"]),
                "title": str(rec["title"]),
                "authority_score": str(rec["authority_score"]),
                "relevance_score": str(rec["relevance_score"]),
                "authority_reason": str(rec["authority_reason"]),
                "relevance_reason": str(rec["relevance_reason"]),
                "search_engine": str(rec["search_engine"]),
            }
            for rec in self.all_results_with_scores
            if rec["authority_score"] == self.filter_authority_score
            and rec["relevance_score"] == self.filter_relevance_score
        ]

        if filtered_results:
            df_filtered = pd.DataFrame(filtered_results)
            csv_path_3 = os.path.join(self.output_dir, f"filtered_qna{part_suffix}")
            df_filtered.to_csv(csv_path_3, index=False, encoding="utf-8-sig")
            logger.info(
                "âœ“ è¾“å‡ºåˆ†ç‰‡æ–‡ä»¶: %s (%d æ¡è®°å½•, ç­›é€‰æ¡ä»¶: authority_score=%d, relevance_score=%d)",
                csv_path_3,
                len(df_filtered),
                self.filter_authority_score,
                self.filter_relevance_score,
            )
        else:
            logger.info(
                "å½“å‰æ‰¹æ¬¡æ— ç¬¦åˆæ¡ä»¶çš„é«˜è´¨é‡ç»“æœ (authority_score=%d, relevance_score=%d)",
                self.filter_authority_score,
                self.filter_relevance_score,
            )

        # å†™å®Œåé‡ç½®æ‰¹æ¬¡ç¼“å­˜
        self._reset_chunk_state()

    def process_dataframe(self, df: pd.DataFrame) -> None:
        from tqdm import tqdm

        if "query" not in df.columns:
            raise ValueError("input missing required column 'query'")
        rows = df.to_dict(orient="records")

        if not rows:
            logger.info("âœ… æ‰€æœ‰queryå·²å¤„ç†å®Œæˆï¼")
            return

        # å°†æ•°æ®æŒ‰ checkpoint_interval è¿›è¡Œåˆ†æ‰¹å¤„ç†ï¼ˆ0 è¡¨ç¤ºå•æ‰¹å¤„ç†æ‰€æœ‰æ•°æ®ï¼‰
        chunk_size = self.checkpoint_interval if self.checkpoint_interval > 0 else len(rows)
        chunk_size = max(1, chunk_size)
        total_chunks = (len(rows) + chunk_size - 1) // chunk_size

        for chunk_index, start in enumerate(range(0, len(rows), chunk_size), start=1):
            chunk_rows = rows[start:start + chunk_size]
            if not chunk_rows:
                continue

            self._reset_chunk_state()
            batch_tag = f"(æ‰¹æ¬¡ {chunk_index}/{total_chunks})"
            logger.info("é˜¶æ®µ1: å¼€å§‹å…ƒæœç´¢ %sï¼Œå…± %d ä¸ªquery", batch_tag, len(chunk_rows))
            chunk_search_results: List[SearchResult] = []

            with ThreadPoolExecutor(max_workers=self.max_workers) as executor:
                search_futures = {
                    executor.submit(
                        self.fetch_results,
                        row.get("query", ""),
                        row.get("type"),
                    ): row
                    for row in chunk_rows
                }

                with tqdm(
                    total=len(chunk_rows),
                    desc=f"ğŸ“¡ å…ƒæœç´¢è¿›åº¦ {batch_tag}",
                    unit="query",
                    leave=True,
                ) as pbar:
                    for future in as_completed(search_futures):
                        results = future.result()
                        row = search_futures[future]
                        query = row.get("query", "")

                        if results:
                            chunk_search_results.extend(results)

                        pbar.update(1)

            logger.info("âœ“ å…ƒæœç´¢å®Œæˆ %sï¼Œå…±è·å– %d æ¡ç»“æœ", batch_tag, len(chunk_search_results))

            logger.info("é˜¶æ®µ2: å¼€å§‹LLMæ‰“åˆ† %s", batch_tag)
            if not chunk_search_results:
                logger.warning("å½“å‰æ‰¹æ¬¡æ²¡æœ‰æœç´¢ç»“æœï¼Œè·³è¿‡LLMæ‰“åˆ† %s", batch_tag)
            else:
                with ThreadPoolExecutor(max_workers=self.max_workers) as executor:
                    scoring_futures = [
                        executor.submit(self.score_single_result, result, rank)
                        for rank, result in enumerate(chunk_search_results, start=1)
                    ]

                    with tqdm(
                        total=len(scoring_futures),
                        desc=f"ğŸ¤– LLMæ‰“åˆ†è¿›åº¦ {batch_tag}",
                        unit="æ¡",
                        leave=True,
                    ) as pbar:
                        for scoring_future in as_completed(scoring_futures):
                            scored = scoring_future.result()
                            self._collect_scored_result(scored)
                            pbar.update(1)

                logger.info("âœ“ LLMæ‰“åˆ†å®Œæˆ %s", batch_tag)

            if not chunk_search_results:
                continue

            if self.checkpoint_interval > 0:
                logger.info("æ‰¹æ¬¡å¤„ç†å®Œæˆï¼Œå¼€å§‹ä¿å­˜Checkpoint %s", batch_tag)
                self.save_checkpoint()

            self._write_csv_part(chunk_index)

    def process_inputs(self, input_paths: List[str]) -> None:
        logger.info("=" * 60)
        logger.info("è¯»å– %d ä¸ªparquetæ–‡ä»¶...", len(input_paths))

        # è¯»å–å¹¶åˆå¹¶æ‰€æœ‰parquetæ–‡ä»¶
        all_dfs = []
        for path in input_paths:
            df = self.storage_client.read_parquet(path)
            all_dfs.append(df)

        # åˆå¹¶æ‰€æœ‰DataFrame
        merged_df = pd.concat(all_dfs, ignore_index=True)
        total_queries = len(merged_df)

        logger.info("âœ“ è¯»å–å®Œæˆ: %d ä¸ªæ–‡ä»¶, å…± %d æ¡query", len(input_paths), total_queries)
        logger.info("=" * 60)
        logger.info("")

        # å¯¹æ‰€æœ‰queryè¿›è¡Œmetasearchå’ŒLLMæ‰“åˆ†
        self.process_dataframe(merged_df)

        logger.info("")
        logger.info("=" * 60)
        logger.info("æ‰€æœ‰å¤„ç†å®Œæˆï¼")
        logger.info("=" * 60)

    def flush_outputs(self, authority_prefix: str, qna_prefix: str, date_str: str) -> None:
        if self.csv_part_index > 0:
            logger.info("å·²æŒ‰æ‰¹è¾“å‡ºCSV/Parquetåˆ†ç‰‡ï¼Œflush_outputsè·³è¿‡æ±‡æ€»è¾“å‡º")
            return

        if authority_prefix:
            authority_path = build_output_path(authority_prefix, date_str, "authority_hosts.parquet")
            df_hosts = pd.DataFrame(
                [
                    {"host": host, "authority_score": score}
                    for host, score in self.authority_hosts.items()
                ]
            )
            self.storage_client.write_parquet(df_hosts, authority_path)
            logger.info("authority hosts written to %s", authority_path)

        if qna_prefix:
            qna_path = build_output_path(qna_prefix, date_str, "authority_qna.parquet")
            df_qna = pd.DataFrame(self.qna_records)
            self.storage_client.write_parquet(df_qna, qna_path)
            logger.info("authority qna written to %s", qna_path)

    def flush_outputs_csv(
        self,
        output_dir: str,
        filter_authority_score: int = 4,
        filter_relevance_score: int = 2,
    ) -> None:
        """
        è¾“å‡º4ä¸ªCSVæ–‡ä»¶ï¼š
        0. metasearch_results.csv - å…ƒæœç´¢åŸå§‹ç»“æœï¼ˆå¯ç”¨äºåç»­å•ç‹¬æ‰“åˆ†ï¼‰
        1. all_results_with_scores.csv - æ‰€æœ‰ç»“æœå¸¦è¯„åˆ†
        2. authority_hosts.csv - æƒå¨hoståˆ—è¡¨
        3. filtered_qna.csv - ç­›é€‰åçš„é«˜è´¨é‡ç»“æœï¼ˆauthority_score=filter_authority_score ä¸” relevance_score=filter_relevance_scoreï¼‰
        """
        import os

        if self.csv_part_index > 0:
            logger.info(
                "å·²è¾“å‡º %d ä¸ªCSVåˆ†ç‰‡ (metasearch_results_partXXX.csv ç­‰)ï¼Œè·³è¿‡æœ€ç»ˆæ±‡æ€»",
                self.csv_part_index,
            )
            return

        os.makedirs(output_dir, exist_ok=True)

        # æ–‡ä»¶0ï¼šå…ƒæœç´¢åŸå§‹ç»“æœï¼ˆç”¨äºåç»­å•ç‹¬æ‰“åˆ†ï¼Œæ‰€æœ‰å­—æ®µè½¬ä¸ºstrï¼‰
        if self.metasearch_results:
            df_metasearch = pd.DataFrame(self.metasearch_results)
            df_metasearch = df_metasearch.astype(str)
            csv_path_0 = os.path.join(output_dir, "metasearch_results.csv")
            df_metasearch.to_csv(csv_path_0, index=False, encoding="utf-8-sig")
            logger.info("âœ“ è¾“å‡ºæ–‡ä»¶0: %s (%d æ¡è®°å½•) - å…ƒæœç´¢åŸå§‹ç»“æœ", csv_path_0, len(df_metasearch))
        else:
            logger.warning("æ²¡æœ‰å…ƒæœç´¢ç»“æœï¼Œè·³è¿‡æ–‡ä»¶0")

        # æ–‡ä»¶1ï¼šæ‰€æœ‰ç»“æœå¸¦è¯„åˆ†ï¼ˆæ‰€æœ‰å­—æ®µè½¬ä¸ºstrï¼‰
        if self.all_results_with_scores:
            df_all = pd.DataFrame(self.all_results_with_scores)
            df_all = df_all.astype(str)
            csv_path_1 = os.path.join(output_dir, "all_results_with_scores.csv")
            df_all.to_csv(csv_path_1, index=False, encoding="utf-8-sig")
            logger.info("âœ“ è¾“å‡ºæ–‡ä»¶1: %s (%d æ¡è®°å½•)", csv_path_1, len(df_all))
        else:
            logger.warning("æ²¡æœ‰æœç´¢ç»“æœï¼Œè·³è¿‡æ–‡ä»¶1")

        # æ–‡ä»¶2ï¼šæƒå¨hoståˆ—è¡¨ï¼ˆæ·»åŠ authority_reasonï¼Œæ‰€æœ‰å­—æ®µè½¬ä¸ºstrï¼‰
        if self.authority_hosts:
            df_hosts = pd.DataFrame([
                {
                    "host": host,
                    "authority_score": info["authority_score"],
                    "authority_reason": info["authority_reason"]
                }
                for host, info in self.authority_hosts.items()
            ]).sort_values("authority_score", ascending=False)
            df_hosts = df_hosts.astype(str)
            csv_path_2 = os.path.join(output_dir, "authority_hosts.csv")
            df_hosts.to_csv(csv_path_2, index=False, encoding="utf-8-sig")
            logger.info("âœ“ è¾“å‡ºæ–‡ä»¶2: %s (%d ä¸ªæƒå¨host)", csv_path_2, len(df_hosts))
        else:
            logger.warning("æ²¡æœ‰æƒå¨hostï¼Œè·³è¿‡æ–‡ä»¶2")

        # æ–‡ä»¶3ï¼šç­›é€‰åçš„é«˜è´¨é‡ç»“æœï¼ˆè°ƒæ•´å­—æ®µé¡ºåºï¼Œæ‰€æœ‰å­—æ®µè½¬ä¸ºstrï¼‰
        # ç­›é€‰æ¡ä»¶ï¼šauthority_score = filter_authority_score ä¸” relevance_score = filter_relevance_score
        if self.all_results_with_scores:
            filtered_results = [
                {
                    "query": str(rec["query"]),
                    "url": str(rec["url"]),
                    "content": str(rec["content"]),
                    "title": str(rec["title"]),
                    "authority_score": str(rec["authority_score"]),
                    "relevance_score": str(rec["relevance_score"]),
                    "authority_reason": str(rec["authority_reason"]),
                    "relevance_reason": str(rec["relevance_reason"]),
                    "search_engine": str(rec["search_engine"]),
                }
                for rec in self.all_results_with_scores
                if rec["authority_score"] == filter_authority_score
                and rec["relevance_score"] == filter_relevance_score
            ]

            if filtered_results:
                df_filtered = pd.DataFrame(filtered_results)
                csv_path_3 = os.path.join(output_dir, "filtered_qna.csv")
                df_filtered.to_csv(csv_path_3, index=False, encoding="utf-8-sig")
                logger.info(
                    "âœ“ è¾“å‡ºæ–‡ä»¶3: %s (%d æ¡è®°å½•, ç­›é€‰æ¡ä»¶: authority_score=%d, relevance_score=%d)",
                    csv_path_3,
                    len(df_filtered),
                    filter_authority_score,
                    filter_relevance_score,
                )
            else:
                logger.warning(
                    "æ²¡æœ‰ç¬¦åˆç­›é€‰æ¡ä»¶çš„ç»“æœ (authority_score=%d, relevance_score=%d)ï¼Œè·³è¿‡æ–‡ä»¶3",
                    filter_authority_score,
                    filter_relevance_score,
                )
        else:
            logger.warning("æ²¡æœ‰æœç´¢ç»“æœï¼Œè·³è¿‡æ–‡ä»¶3")
