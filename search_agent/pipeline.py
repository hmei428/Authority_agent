import logging
from concurrent.futures import ThreadPoolExecutor, as_completed
from dataclasses import dataclass
from typing import Dict, Iterable, List, Optional
from urllib.parse import urlparse

import pandas as pd

from .scoring import AuthorityScorer, RelevanceScorer
from .search_client import MetaSearchClient
from .storage import StorageClient, build_output_path

logger = logging.getLogger(__name__)


@dataclass
class SearchResult:
    query: str
    query_type: Optional[str]
    url: str
    title: str
    content: str
    host: str


class AuthorityAgent:
    def __init__(
        self,
        search_client: MetaSearchClient,
        storage_client: StorageClient,
        topk: int,
        authority_threshold: int,
        relevance_threshold: int,
        max_workers: int,
        score_authority: AuthorityScorer,
        score_relevance: RelevanceScorer,
    ) -> None:
        self.search_client = search_client
        self.storage_client = storage_client
        self.topk = topk
        self.authority_threshold = authority_threshold
        self.relevance_threshold = relevance_threshold
        self.max_workers = max_workers
        self.score_authority = score_authority
        self.score_relevance = score_relevance
        self.authority_hosts: Dict[str, int] = {}
        self.qna_records: List[Dict[str, str]] = []
        self.all_results_with_scores: List[Dict] = []  # æ–°å¢ï¼šå­˜å‚¨æ‰€æœ‰ç»“æœå¸¦è¯„åˆ†
        self.result_rank_counter: Dict[str, int] = {}  # æ–°å¢ï¼šè¿½è¸ªæ¯ä¸ªqueryçš„ç»“æœæ’åº
        self.metasearch_results: List[Dict] = []  # æ–°å¢ï¼šå­˜å‚¨metasearchåŸå§‹ç»“æœï¼ˆç”¨äºåç»­å•ç‹¬æ‰“åˆ†ï¼‰

        # ç»Ÿè®¡ä¿¡æ¯
        self.stats = {
            "total_queries": 0,
            "search_success": 0,
            "search_failed": 0,
            "authority_score_failed": 0,
            "relevance_score_failed": 0,
        }

    def fetch_results(self, query: str, query_type: Optional[str]) -> List[SearchResult]:
        self.stats["total_queries"] += 1
        try:
            items: Iterable[Dict] = self.search_client.search(query)
            self.stats["search_success"] += 1
        except Exception as exc:  # noqa: BLE001
            self.stats["search_failed"] += 1
            logger.warning("å…ƒæœç´¢å¤±è´¥ (query=%s): %s", query, exc)
            return []

        results: List[SearchResult] = []
        for rank, item in enumerate(list(items)[: self.topk], start=1):
            url = item.get("link") or ""
            title = item.get("title") or ""
            content = item.get("content") or ""
            host = urlparse(url).netloc
            if not url or not host:
                continue

            # ä¿å­˜åˆ°metasearch_resultsï¼ˆç”¨äºåç»­å•ç‹¬æ‰“åˆ†ï¼‰
            self.metasearch_results.append({
                "query": query,
                "rank": rank,
                "url": url,
                "title": title,
                "content": content,
                "host": host,
            })

            results.append(
                SearchResult(
                    query=query,
                    query_type=query_type,
                    url=url,
                    title=title,
                    content=content,
                    host=host,
                )
            )
        return results

    def score_single_result(self, result: SearchResult, rank: int) -> Dict:
        """
        å¯¹å•ä¸ªæœç´¢ç»“æœè¿›è¡Œæ‰“åˆ†ï¼ˆå¹¶å‘å®‰å…¨ï¼‰
        åªåšæ‰“åˆ†ï¼Œä¸ä¿®æ”¹å…±äº«çŠ¶æ€
        """
        # å¯¹hostè¿›è¡Œæƒå¨æ€§æ‰“åˆ†
        try:
            authority_score = self.score_authority(result.host, result.title, result.content)
        except Exception as exc:  # noqa: BLE001
            logger.warning("æƒå¨æ€§æ‰“åˆ†å¤±è´¥ (host=%s): %s", result.host, exc)
            authority_score = 0

        # å¯¹query-contentè¿›è¡Œç›¸å…³æ€§æ‰“åˆ†
        try:
            relevance_score = self.score_relevance(
                result.query, result.title, result.content
            )
        except Exception as exc:  # noqa: BLE001
            logger.warning("ç›¸å…³æ€§æ‰“åˆ†å¤±è´¥ (query=%s): %s", result.query, exc)
            relevance_score = -1

        return {
            "result": result,
            "rank": rank,
            "authority_score": authority_score,
            "relevance_score": relevance_score,
        }

    def evaluate_result(self, result: SearchResult, rank: int) -> None:
        """
        è¯„ä¼°å•ä¸ªç»“æœå¹¶æ›´æ–°ç»Ÿè®¡ï¼ˆéå¹¶å‘ç‰ˆæœ¬ï¼Œä¿ç•™ç”¨äºå…¼å®¹ï¼‰
        """
        scored = self.score_single_result(result, rank)
        self._collect_scored_result(scored)

    def _collect_scored_result(self, scored: Dict) -> None:
        """
        æ”¶é›†æ‰“åˆ†åçš„ç»“æœåˆ°å…±äº«æ•°æ®ç»“æ„ï¼ˆçº¿ç¨‹å®‰å…¨ï¼Œéœ€è¦åœ¨ä¸»çº¿ç¨‹è°ƒç”¨ï¼‰
        """
        result = scored["result"]
        rank = scored["rank"]
        authority_score = scored["authority_score"]
        relevance_score = scored["relevance_score"]

        # æ›´æ–°å¤±è´¥ç»Ÿè®¡
        if authority_score == 0:
            self.stats["authority_score_failed"] += 1
        if relevance_score == -1:
            self.stats["relevance_score_failed"] += 1

        # å­˜å‚¨æ‰€æœ‰ç»“æœï¼ˆå¸¦è¯„åˆ†ï¼‰
        self.all_results_with_scores.append({
            "query": result.query,
            "rank": rank,
            "url": result.url,
            "title": result.title,
            "content": result.content,
            "host": result.host,
            "authority_score": authority_score,
            "relevance_score": relevance_score,
        })

        # æ”¶é›†æƒå¨host
        if authority_score >= self.authority_threshold:
            existing = self.authority_hosts.get(result.host, authority_score)
            self.authority_hosts[result.host] = max(existing, authority_score)

            # æ”¶é›†é«˜æƒå¨é«˜ç›¸å…³çš„ç»“æœ
            if relevance_score >= self.relevance_threshold:
                key = (result.query, result.url)
                # å»é‡åŒ query-url ç»„åˆ
                if not any(
                    (rec["query"], rec["url"]) == key for rec in self.qna_records
                ):
                    self.qna_records.append(
                        {
                            "query": result.query,
                            "type": result.query_type or "",
                            "url": result.url,
                            "title": result.title,
                            "content": result.content,
                            "authority_score": authority_score,
                            "relevance_score": relevance_score,
                        }
                    )

    def process_dataframe(self, df: pd.DataFrame) -> None:
        from tqdm import tqdm

        if "query" not in df.columns:
            raise ValueError("input missing required column 'query'")
        rows = df.to_dict(orient="records")

        # ========================================
        # é˜¶æ®µ1ï¼šå¹¶å‘æ‰§è¡Œæ‰€æœ‰queryçš„å…ƒæœç´¢
        # ========================================
        logger.info("é˜¶æ®µ1: å¼€å§‹å…ƒæœç´¢...")
        all_search_results = []  # å­˜å‚¨æ‰€æœ‰æœç´¢ç»“æœ

        with ThreadPoolExecutor(max_workers=self.max_workers) as executor:
            search_futures = {
                executor.submit(
                    self.fetch_results,
                    row.get("query", ""),
                    row.get("type"),
                ): row
                for row in rows
            }

            # æ˜¾ç¤ºå…ƒæœç´¢è¿›åº¦
            for future in tqdm(
                as_completed(search_futures),
                total=len(search_futures),
                desc="ğŸ“¡ å…ƒæœç´¢è¿›åº¦",
                unit="query",
                leave=True
            ):
                results = future.result()
                if results:
                    all_search_results.extend(results)

        logger.info("âœ“ å…ƒæœç´¢å®Œæˆï¼Œå…±è·å– %d æ¡ç»“æœ", len(all_search_results))

        # ========================================
        # é˜¶æ®µ2ï¼šå¹¶å‘å¯¹æ‰€æœ‰ç»“æœè¿›è¡ŒLLMæ‰“åˆ†
        # ========================================
        logger.info("é˜¶æ®µ2: å¼€å§‹LLMæ‰“åˆ†...")

        with ThreadPoolExecutor(max_workers=self.max_workers) as executor:
            scoring_futures = [
                executor.submit(self.score_single_result, result, rank)
                for rank, result in enumerate(all_search_results, start=1)
            ]

            # æ˜¾ç¤ºæ‰“åˆ†è¿›åº¦
            for scoring_future in tqdm(
                as_completed(scoring_futures),
                total=len(scoring_futures),
                desc="ğŸ¤– LLMæ‰“åˆ†è¿›åº¦",
                unit="æ¡",
                leave=True
            ):
                scored = scoring_future.result()
                self._collect_scored_result(scored)

        logger.info("âœ“ LLMæ‰“åˆ†å®Œæˆ")

    def process_inputs(self, input_paths: List[str]) -> None:
        logger.info("=" * 60)
        logger.info("è¯»å– %d ä¸ªparquetæ–‡ä»¶...", len(input_paths))

        # è¯»å–å¹¶åˆå¹¶æ‰€æœ‰parquetæ–‡ä»¶
        all_dfs = []
        for path in input_paths:
            df = self.storage_client.read_parquet(path)
            all_dfs.append(df)

        # åˆå¹¶æ‰€æœ‰DataFrame
        merged_df = pd.concat(all_dfs, ignore_index=True)
        total_queries = len(merged_df)

        logger.info("âœ“ è¯»å–å®Œæˆ: %d ä¸ªæ–‡ä»¶, å…± %d æ¡query", len(input_paths), total_queries)
        logger.info("=" * 60)
        logger.info("")

        # å¯¹æ‰€æœ‰queryè¿›è¡Œmetasearchå’ŒLLMæ‰“åˆ†
        self.process_dataframe(merged_df)

        logger.info("")
        logger.info("=" * 60)
        logger.info("æ‰€æœ‰å¤„ç†å®Œæˆï¼")
        logger.info("=" * 60)

    def flush_outputs(self, authority_prefix: str, qna_prefix: str, date_str: str) -> None:
        if authority_prefix:
            authority_path = build_output_path(authority_prefix, date_str, "authority_hosts.parquet")
            df_hosts = pd.DataFrame(
                [
                    {"host": host, "authority_score": score}
                    for host, score in self.authority_hosts.items()
                ]
            )
            self.storage_client.write_parquet(df_hosts, authority_path)
            logger.info("authority hosts written to %s", authority_path)

        if qna_prefix:
            qna_path = build_output_path(qna_prefix, date_str, "authority_qna.parquet")
            df_qna = pd.DataFrame(self.qna_records)
            self.storage_client.write_parquet(df_qna, qna_path)
            logger.info("authority qna written to %s", qna_path)

    def flush_outputs_csv(
        self,
        output_dir: str,
        filter_authority_score: int = 4,
        filter_relevance_score: int = 2,
    ) -> None:
        """
        è¾“å‡º4ä¸ªCSVæ–‡ä»¶ï¼š
        0. metasearch_results.csv - å…ƒæœç´¢åŸå§‹ç»“æœï¼ˆå¯ç”¨äºåç»­å•ç‹¬æ‰“åˆ†ï¼‰
        1. all_results_with_scores.csv - æ‰€æœ‰ç»“æœå¸¦è¯„åˆ†
        2. authority_hosts.csv - æƒå¨hoståˆ—è¡¨
        3. filtered_qna.csv - ç­›é€‰åçš„é«˜è´¨é‡ç»“æœï¼ˆauthority_score=filter_authority_score ä¸” relevance_score=filter_relevance_scoreï¼‰
        """
        import os

        os.makedirs(output_dir, exist_ok=True)

        # æ–‡ä»¶0ï¼šå…ƒæœç´¢åŸå§‹ç»“æœï¼ˆç”¨äºåç»­å•ç‹¬æ‰“åˆ†ï¼‰
        if self.metasearch_results:
            df_metasearch = pd.DataFrame(self.metasearch_results)
            csv_path_0 = os.path.join(output_dir, "metasearch_results.csv")
            df_metasearch.to_csv(csv_path_0, index=False, encoding="utf-8-sig")
            logger.info("âœ“ è¾“å‡ºæ–‡ä»¶0: %s (%d æ¡è®°å½•) - å…ƒæœç´¢åŸå§‹ç»“æœ", csv_path_0, len(df_metasearch))
        else:
            logger.warning("æ²¡æœ‰å…ƒæœç´¢ç»“æœï¼Œè·³è¿‡æ–‡ä»¶0")

        # æ–‡ä»¶1ï¼šæ‰€æœ‰ç»“æœå¸¦è¯„åˆ†
        if self.all_results_with_scores:
            df_all = pd.DataFrame(self.all_results_with_scores)
            csv_path_1 = os.path.join(output_dir, "all_results_with_scores.csv")
            df_all.to_csv(csv_path_1, index=False, encoding="utf-8-sig")
            logger.info("âœ“ è¾“å‡ºæ–‡ä»¶1: %s (%d æ¡è®°å½•)", csv_path_1, len(df_all))
        else:
            logger.warning("æ²¡æœ‰æœç´¢ç»“æœï¼Œè·³è¿‡æ–‡ä»¶1")

        # æ–‡ä»¶2ï¼šæƒå¨hoståˆ—è¡¨
        if self.authority_hosts:
            df_hosts = pd.DataFrame([
                {"host": host, "authority_score": score}
                for host, score in self.authority_hosts.items()
            ]).sort_values("authority_score", ascending=False)
            csv_path_2 = os.path.join(output_dir, "authority_hosts.csv")
            df_hosts.to_csv(csv_path_2, index=False, encoding="utf-8-sig")
            logger.info("âœ“ è¾“å‡ºæ–‡ä»¶2: %s (%d ä¸ªæƒå¨host)", csv_path_2, len(df_hosts))
        else:
            logger.warning("æ²¡æœ‰æƒå¨hostï¼Œè·³è¿‡æ–‡ä»¶2")

        # æ–‡ä»¶3ï¼šç­›é€‰åçš„é«˜è´¨é‡ç»“æœ
        # ç­›é€‰æ¡ä»¶ï¼šauthority_score = filter_authority_score ä¸” relevance_score = filter_relevance_score
        if self.all_results_with_scores:
            filtered_results = [
                {
                    "query": rec["query"],
                    "url": rec["url"],
                    "title": rec["title"],
                    "content": rec["content"],
                    "authority_score": rec["authority_score"],
                    "relevance_score": rec["relevance_score"],
                }
                for rec in self.all_results_with_scores
                if rec["authority_score"] == filter_authority_score
                and rec["relevance_score"] == filter_relevance_score
            ]

            if filtered_results:
                df_filtered = pd.DataFrame(filtered_results)
                csv_path_3 = os.path.join(output_dir, "filtered_qna.csv")
                df_filtered.to_csv(csv_path_3, index=False, encoding="utf-8-sig")
                logger.info(
                    "âœ“ è¾“å‡ºæ–‡ä»¶3: %s (%d æ¡è®°å½•, ç­›é€‰æ¡ä»¶: authority_score=%d, relevance_score=%d)",
                    csv_path_3,
                    len(df_filtered),
                    filter_authority_score,
                    filter_relevance_score,
                )
            else:
                logger.warning(
                    "æ²¡æœ‰ç¬¦åˆç­›é€‰æ¡ä»¶çš„ç»“æœ (authority_score=%d, relevance_score=%d)ï¼Œè·³è¿‡æ–‡ä»¶3",
                    filter_authority_score,
                    filter_relevance_score,
                )
        else:
            logger.warning("æ²¡æœ‰æœç´¢ç»“æœï¼Œè·³è¿‡æ–‡ä»¶3")
